# This module provides service functionality to app.py
# working directory is src folder.

import json
import sys
import os

import pandas as pd
import numpy as np
from typing import List
from sklearn.cluster import KMeans
from collections import defaultdict

from src.train.result_model import TResult
from src.train.store import StoreData
from src.util import language_dict,language_list,db_config
from src.train.train_cluster import load_model
from src.train.train_model import UdpipeTrain

try:
    store_data = StoreData(db_config['user'],
                           db_config['password'],
                           db_host=db_config['db_host'],
                           db_name=db_config['db_name'])
    cnx = store_data.db_connect()
    cursor = cnx.cursor()
except Exception as ex:
    print('logging in database error %s' % ex)

POS_COLUMN_INDEX,SENTENCE_COLUMN_INDEX = 2,6


class AppService(object):
    def __init__(self):
        self.pos_dict = None
        self.sel_result = None
        self.udt_pre_model = None

    def config_udpipe(self,language_name):
        # first loading udpipe to segement word for each sentence
        # TODO: once getting language_name, then to find the related udpipe and corpus
        # all these need to be at preprocessed level
        self.udt_pre_model = UdpipeTrain(language_name,
                                         r'C:\Users\haris\Desktop\wordFinder\english-ewt-ud-2.5-191206.udpipe',
                                         r'C:\Users\haris\Desktop\wordFinder\haris.txt')
        return self

    def find_service(self,language_name: str,sel_word: str):
        """This method get results from database by specified language_name and input word
        assgin value to self.pos_dict and self.sel_result
        :param language_name:
        :param sel_word:
        :return: None
        """
        # select
        sql_str = "select * from " + language_name + "_wordpos as w left join " + language_name + "_sentences as s on " \
                                                                                                  "w.sentence = s.id " \
                                                                                                  "where w.word = %s "
        try:
            cursor.execute(sql_str, (sel_word,))
            self.sel_result = cursor.fetchall()
            cnx.commit()
        except Exception as e:
            print(e)

        # convert to data structure following
        # sel_result = (("sink", "NOUN", ["Don't just leave your dirty plates in the sink!"]),
        #                ("sink", "VERB", ["The wheels, started to sink into the mud.", "How could you sink so low?"]))
        self.pos_dict = defaultdict(list)
        for row in self.sel_result:
            pos_sentences = self.pos_dict[row[POS_COLUMN_INDEX]]
            if row[SENTENCE_COLUMN_INDEX] not in pos_sentences:
                pos_sentences.append(row[SENTENCE_COLUMN_INDEX])
        self.sel_result = [(sel_word,k,self.pos_dict[k]) for k in self.pos_dict]

    def database(self):
        self.store_data = StoreData(db_config['user'],
                                    db_config['password'],
                                    db_host=db_config['db_host'],
                                    db_name=db_config['db_name'])
        self.cursor = self.store_data.db_connect().cursor()
        query_info = "SELECT sentence FROM english_sentences"
        self.cursor.execute(query_info)
        sentences_df = pd.DataFrame(self.cursor.fetchall(),columns=['Sentences'])
        return sentences_df

    def clusteringData(self):
        self.store_data = StoreData(db_config['user'],
                                    db_config['password'],
                                    db_host=db_config['db_host'],
                                    db_name=db_config['db_name'])
        self.cursor = self.store_data.db_connect().cursor()
        query_info = "SELECT sentence FROM english_sentences"
        self.cursor.execute(query_info)
        sentences_dataframe = pd.DataFrame(self.cursor.fetchall(),columns=['Sentences'])
        return sentences_dataframe

    def cluster_sentences(self, language_name: str, save_path: str, sentences: List[str], n_clusters: int) -> List[str]:
        """
        cluster sentences to get examples
        :param language_name:
        :param save_path: the saved path for our cluster model trained well
        :param sentences:
        :param n_clusters:
        :return:
        """
        n_clusters = int(n_clusters)
        if n_clusters <=0:
            print("Parameter is Invalid")
            return
        if n_clusters > len(sentences):
            # TODO add log
            print('number of cluster bigger than sentences count')
            return
        # first loading model
        word2vec_model = load_model(save_path)
        # second geting vectors for one sentence
        sent_vectors = []
        default_dimn = 100
        # iterator to sentence
        for sent in sentences:
            words = self.udt_pre_model.word_segmentation(sent)
            word_vectors = []
            # iterator to word
            for word in words:
                if word in word2vec_model.wv:
                    word_vectors.append(word2vec_model.wv[word])
                else:  # not in dict, fill 0
                    word_vectors.append([0] * default_dimn)

            to_array = np.array(word_vectors)
            sent_vectors.append(to_array.mean(axis=0).tolist())

        # third using kmeans to cluster
        kmeans = KMeans(n_clusters=n_clusters,random_state=0).fit(sent_vectors)
        labels = kmeans.labels_
        # fourth select one sentence with each label
        tmp_labels,examples = [],[]
        for sent,label in zip(sentences,labels):
            if label not in tmp_labels:
                tmp_labels.append(label)
                examples.append(sent)
            if len(examples) == n_clusters:
                break
        # add bottom logic for cluster
        if len(examples) < n_clusters:
            for sent in sentences:
                if sent not in examples:
                    examples.append(sent)
                if len(examples) >= n_clusters:
                    break

        return examples


if __name__ == "__main__":
    # get word vector for one sentence
    language_name = 'English'
    sentences = [
        'Tohru shows great loyalty to whoever he stands by, even back to the time when he was an Enforcer for the Dark Hand.',
        'The Earth Demon, Dai Gui resembles a large minotaur(with the face of a guardian lion) with great strength.',
        'Al Mulock was the great-grandson of Sir William Mulock(1843â€“1944), the former Canadian Postmaster - General.',
        'Though his surviving images are scarce, his importance to the early history of photography in Asia is great.']

    # first loading udpipe to segement word for each sentence
    udt_english = UdpipeTrain(language_list[1],
                              r'C:\Users\haris\Desktop\wordFinder\english-ewt-ud-2.5-191206.udpipe',
                              r'C:\Users\haris\Desktop\wordFinder\haris.txt')

    cluster_result = AppService().config_udpipe(language_name).cluster_sentences(language_name,sentences,2)
    print("two examples sentences: \n")
    print(cluster_result)
